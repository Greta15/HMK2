# -*- coding: utf-8 -*-
"""HW2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/15rNpvtW4uBs9l-uVdun-KfawOr-lFQK4
"""

import time
import requests
import numpy as np
import pandas as pd
import re
!pip install scrapy
from scrapy.http import TextResponse

"""# ***Problem 1(Scraping quotes)***"""

URL="http://quotes.toscrape.com/"

page=requests.get(URL)

type(page)

page.url

response= TextResponse(body=page.text,url=URL,encoding="utf-8")

def request(url):
    page = requests.get(url)
    response = TextResponse(body=page.text,url=url,encoding="utf-8")
    return response

def quote_scraper(response):
    page = requests.get(url)
    response= TextResponse(body=page.text,url=url,encoding="utf-8")
    author = response.css("small.author::text").extract()
    quotes = response.css("div.quote > span.text::text").extract()
    div_tags =response.css("div.tags ")
    tags = [i.css("a.tag::text").extract() for i in response.css("div.tags")]
    base_url="http://quotes.toscrape.com/"
    rel_hyperlinks=response.css("small.author~a::attr(href)").extract()
    hyperlink=[base_url+i for i in rel_hyperlinks]
    return pd.DataFrame({"quotes":quotes,"author":author,"tags":tags,"author_page":hyperlink})

quotes = []
url = "http://quotes.toscrape.com/" 
while True:
    response =request(url)
    quotes.append(quote_scraper(response))
    url_for_next_page = response.css("li.next > a::attr(href)").extract_first()
    if url_for_next_page:
        url = response.urljoin(url_for_next_page)
    else:
        break

quotes = pd.concat(quotes)
quotes

"""# ***Problem 2(Scraping movies)***"""

URL="https://www.imdb.com/chart/moviemeter"

page=requests.get(URL)

response= TextResponse(body=page.text,url=URL,encoding="utf-8")

response.css("td>span.secondaryInfo::text").extract()

response.css("td.titleColumn > a::text").extract()

def scraping_movies(url):
    page = requests.get(url)
    response = TextResponse(body=page.text,url=url,encoding="utf-8")
    year_of_release = response.css("td >span.secondaryInfo::text").extract()
    year = [i.replace("(" , "").replace(")" , "") for i in year_of_release]
    movie_title = response.css("td.titleColumn > a ::text").extract()
    rate = response.css("td.ratingColumn.imdbRating")
    imdb_rate = [i.css('strong::text').extract_first() for i in response.css("td.ratingColumn.imdbRating")]
    movie_link = response.css("td.titleColumn ::attr(href)").extract()
    base_url="https://www.imdb.com/chart/moviemeter/"
    hyperlink = 'https://www.imdb.com/'
    movie_hyperlink = [hyperlink+i for i in movie_link]
    div_rank = response.css("div.velocity")
    rank= [i.css("::text").extract_first() for i in div_rank]  
    return pd.DataFrame({"movie_title":movie_title,  "year":year, "imdb_rate":imdb_rate, "movie_hyperlink": movie_hyperlink , 'rank':rank,})
imdbmovies =scraping_movies(url = "https://www.imdb.com/chart/moviemeter/")
imdbmovies

def booksto_scrape(url):
    page=requests.get(url)
    response=TextResponse(body=page.text,url=url,encoding="utf-8")

    book_title=response.css("h3>a::attr(title)").extract()
    book_rating=response.css("p[class^='star-rating']::attr(class)").extract()
    p_price = response.css("p[class^='price_color']::attr(class)").extract()
    price = [i.replace("Ã‚", "") for i in p_price]
    instock_or_not  =response.css("p.price_color ~ p.instock::attr(class)").extract()
    instock = [i.replace("availability", " ") for i in instock_or_not]
    base_url = "http://books.toscrape.com/catalogue/"
    book_page_URL = response.css("h3 >a::attr(href)").extract()
    book_page_URL2 = [base_url + i for i in book_page_URL ]
    book_picture_URL = response.css("img::attr(src)").extract()
    book_picture_URL2 = [base_url + i for i in book_picture_URL] 
    rating = []
    for i in book_rating:
      rating.append(i.replace("star-rating", ""))
    return pd.DataFrame({"book_title":book_title, "rating":rating, "price":price,"instock":instock, "book_page_URL2":book_page_URL2, "book_picture_URL2":book_picture_URL2})  
bookslist = []
for i in range(1,11561000):
    current_page =booksto_scrape(url = f"http://books.toscrape.com/catalogue/page-{i}.html")
    if current_page.shape[0] == 0:
        break
    else:
        bookslist.append(current_page)
bookslist = pd.concat(bookslist)
bookslist